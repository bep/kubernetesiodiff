<html>
  <body>
    <h1>
      Running in multiple zones
    </h1>
    <p>
      <a href="https://github.com/kubernetes/website/edit/master/content/en/docs/setup/best-practices/multiple-zones.md" id="editPageButton" target="_blank">
        Edit This Page
      </a>
    </p>
    <h1>
      Running in multiple zones
    </h1>
    <p>
      This page describes how to run a cluster in multiple zones.
    </p>
    <ul id="markdown-toc">
      <li>
        <a href="#introduction">
          Introduction
        </a>
      </li>
      <li>
        <a href="#functionality">
          Functionality
        </a>
      </li>
      <li>
        <a href="#limitations">
          Limitations
        </a>
      </li>
      <li>
        <a href="#walkthrough">
          Walkthrough
        </a>
      </li>
    </ul>
    <h2 id="introduction">
      Introduction
    </h2>
    <p>
      Kubernetes 1.2 adds support for running a single cluster in multiple failure zones
      (GCE calls them simply &ldquo;zones&rdquo;, AWS calls them &ldquo;availability zones&rdquo;, here we&rsquo;ll refer to them as &ldquo;zones&rdquo;).
      This is a lightweight version of a broader Cluster Federation feature (previously referred to by the affectionate
      nickname
      <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multicluster/federation.md">
        &ldquo;Ubernetes&rdquo;
      </a>
      ).
      Full Cluster Federation allows combining separate
      Kubernetes clusters running in different regions or cloud providers
      (or on-premises data centers).  However, many
      users simply want to run a more available Kubernetes cluster in multiple zones
      of their single cloud provider, and this is what the multizone support in 1.2 allows
      (this previously went by the nickname &ldquo;Ubernetes Lite&rdquo;).
    </p>
    <p>
      Multizone support is deliberately limited: a single Kubernetes cluster can run
      in multiple zones, but only within the same region (and cloud provider).  Only
      GCE and AWS are currently supported automatically (though it is easy to
      add similar support for other clouds or even bare metal, by simply arranging
      for the appropriate labels to be added to nodes and volumes).
    </p>
    <h2 id="functionality">
      Functionality
    </h2>
    <p>
      When nodes are started, the kubelet automatically adds labels to them with
      zone information.
    </p>
    <p>
      Kubernetes will automatically spread the pods in a replication controller
      or service across nodes in a single-zone cluster (to reduce the impact of
      failures.)  With multiple-zone clusters, this spreading behavior is
      extended across zones (to reduce the impact of zone failures.)  (This is
      achieved via
      <code>
        SelectorSpreadPriority
      </code>
      ).  This is a best-effort
      placement, and so if the zones in your cluster are heterogeneous
      (e.g. different numbers of nodes, different types of nodes, or
      different pod resource requirements), this might prevent perfectly
      even spreading of your pods across zones. If desired, you can use
      homogeneous zones (same number and types of nodes) to reduce the
      probability of unequal spreading.
    </p>
    <p>
      When persistent volumes are created, the
      <code>
        PersistentVolumeLabel
      </code>
      admission controller automatically adds zone labels to them.  The scheduler (via the
      <code>
        VolumeZonePredicate
      </code>
      predicate) will then ensure that pods that claim a
      given volume are only placed into the same zone as that volume, as volumes
      cannot be attached across zones.
    </p>
    <h2 id="limitations">
      Limitations
    </h2>
    <p>
      There are some important limitations of the multizone support:
    </p>
    <ul>
      <li>
        <p>
          We assume that the different zones are located close to each other in the
          network, so we don&rsquo;t perform any zone-aware routing.  In particular, traffic
          that goes via services might cross zones (even if some pods backing that service
          exist in the same zone as the client), and this may incur additional latency and cost.
        </p>
      </li>
      <li>
        <p>
          Volume zone-affinity will only work with a
          <code>
            PersistentVolume
          </code>
          , and will not
          work if you directly specify an EBS volume in the pod spec (for example).
        </p>
      </li>
      <li>
        <p>
          Clusters cannot span clouds or regions (this functionality will require full
          federation support).
        </p>
      </li>
      <li>
        <p>
          Although your nodes are in multiple zones, kube-up currently builds
          a single master node by default.  While services are highly
          available and can tolerate the loss of a zone, the control plane is
          located in a single zone.  Users that want a highly available control
          plane should follow the
          <a href="/docs/admin/high-availability">
            high availability
          </a>
          instructions.
        </p>
      </li>
    </ul>
    <h3 id="volume-limitations">
      Volume limitations
    </h3>
    <p>
      The following limitations are addressed with
      <a href="/docs/concepts/storage/storage-classes/#volume-binding-mode">
        topology-aware volume binding
      </a>
      .
    </p>
    <ul>
      <li>
        <p>
          StatefulSet volume zone spreading when using dynamic provisioning is currently not compatible with
          pod affinity or anti-affinity policies.
        </p>
      </li>
      <li>
        <p>
          If the name of the StatefulSet contains dashes (&ldquo;-&rdquo;), volume zone spreading
          may not provide a uniform distribution of storage across zones.
        </p>
      </li>
      <li>
        <p>
          When specifying multiple PVCs in a Deployment or Pod spec, the StorageClass
          needs to be configured for a specific single zone, or the PVs need to be
          statically provisioned in a specific zone. Another workaround is to use a
          StatefulSet, which will ensure that all the volumes for a replica are
          provisioned in the same zone.
        </p>
      </li>
    </ul>
    <h2 id="walkthrough">
      Walkthrough
    </h2>
    <p>
      We&rsquo;re now going to walk through setting up and using a multi-zone
      cluster on both GCE &amp; AWS.  To do so, you bring up a full cluster
      (specifying
      <code>
        MULTIZONE=true
      </code>
      ), and then you add nodes in additional zones
      by running
      <code>
        kube-up
      </code>
      again (specifying
      <code>
        KUBE_USE_EXISTING_MASTER=true
      </code>
      ).
    </p>
    <h3 id="bringing-up-your-cluster">
      Bringing up your cluster
    </h3>
    <p>
      Create the cluster as normal, but pass MULTIZONE to tell the cluster to manage multiple zones; creating nodes in us-central1-a.
    </p>
    <p>
      GCE:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -sS https://get.k8s.io | <span style="color:#b8860b">MULTIZONE</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>gce <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>us-central1-a <span style="color:#b8860b">NUM_NODES</span><span style="color:#666">=</span><span style="color:#666">3</span> bash</code></pre>
    </div>
    <p>
      AWS:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -sS https://get.k8s.io | <span style="color:#b8860b">MULTIZONE</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>aws <span style="color:#b8860b">KUBE_AWS_ZONE</span><span style="color:#666">=</span>us-west-2a <span style="color:#b8860b">NUM_NODES</span><span style="color:#666">=</span><span style="color:#666">3</span> bash</code></pre>
    </div>
    <p>
      This step brings up a cluster as normal, still running in a single zone
      (but
      <code>
        MULTIZONE=true
      </code>
      has enabled multi-zone capabilities).
    </p>
    <h3 id="nodes-are-labeled">
      Nodes are labeled
    </h3>
    <p>
      View the nodes; you can see that they are labeled with zone information.
      They are all in
      <code>
        us-central1-a
      </code>
      (GCE) or
      <code>
        us-west-2a
      </code>
      (AWS) so far.  The
      labels are
      <code>
        failure-domain.beta.kubernetes.io/region
      </code>
      for the region,
      and
      <code>
        failure-domain.beta.kubernetes.io/zone
      </code>
      for the zone:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes --show-labels</code></pre>
    </div>
    <p>
      The output is similar to this:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                     STATUS                     ROLES    AGE   VERSION          LABELS
kubernetes-master        Ready,SchedulingDisabled   &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-1,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-master
kubernetes-minion-87j9   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-87j9
kubernetes-minion-9vlv   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-9vlv
kubernetes-minion-a12q   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-a12q</code></pre>
    </div>
    <h3 id="add-more-nodes-in-a-second-zone">
      Add more nodes in a second zone
    </h3>
    <p>
      Let&rsquo;s add another set of nodes to the existing cluster, reusing the
      existing master, running in a different zone (us-central1-b or us-west-2b).
      We run kube-up again, but by specifying
      <code>
        KUBE_USE_EXISTING_MASTER=true
      </code>
      kube-up will not create a new master, but will reuse one that was previously
      created instead.
    </p>
    <p>
      GCE:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBE_USE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">MULTIZONE</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>gce <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>us-central1-b <span style="color:#b8860b">NUM_NODES</span><span style="color:#666">=</span><span style="color:#666">3</span> kubernetes/cluster/kube-up.sh</code></pre>
    </div>
    <p>
      On AWS we also need to specify the network CIDR for the additional
      subnet, along with the master internal IP address:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBE_USE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">MULTIZONE</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>aws <span style="color:#b8860b">KUBE_AWS_ZONE</span><span style="color:#666">=</span>us-west-2b <span style="color:#b8860b">NUM_NODES</span><span style="color:#666">=</span><span style="color:#666">3</span> <span style="color:#b8860b">KUBE_SUBNET_CIDR</span><span style="color:#666">=</span>172.20.1.0/24 <span style="color:#b8860b">MASTER_INTERNAL_IP</span><span style="color:#666">=</span>172.20.0.9 kubernetes/cluster/kube-up.sh</code></pre>
    </div>
    <p>
      View the nodes again; 3 more nodes should have launched and be tagged
      in us-central1-b:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes --show-labels</code></pre>
    </div>
    <p>
      The output is similar to this:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                     STATUS                     ROLES    AGE   VERSION           LABELS
kubernetes-master        Ready,SchedulingDisabled   &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-1,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-master
kubernetes-minion-281d   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-b,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-281d
kubernetes-minion-87j9   Ready                      &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-87j9
kubernetes-minion-9vlv   Ready                      &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-9vlv
kubernetes-minion-a12q   Ready                      &lt;none&gt;   17m   v1.13.0           beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-a12q
kubernetes-minion-pp2f   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-b,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-pp2f
kubernetes-minion-wf8i   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-b,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-wf8i</code></pre>
    </div>
    <h3 id="volume-affinity">
      Volume affinity
    </h3>
    <p>
      Create a volume using the dynamic volume creation (only PersistentVolumes are supported for zone affinity):
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f - <span style="color:#b44">&lt;&lt;EOF
</span><span style="color:#b44">{
</span><span style="color:#b44">  &#34;apiVersion&#34;: &#34;v1&#34;,
</span><span style="color:#b44">  &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;,
</span><span style="color:#b44">  &#34;metadata&#34;: {
</span><span style="color:#b44">    &#34;name&#34;: &#34;claim1&#34;,
</span><span style="color:#b44">    &#34;annotations&#34;: {
</span><span style="color:#b44">        &#34;volume.alpha.kubernetes.io/storage-class&#34;: &#34;foo&#34;
</span><span style="color:#b44">    }
</span><span style="color:#b44">  },
</span><span style="color:#b44">  &#34;spec&#34;: {
</span><span style="color:#b44">    &#34;accessModes&#34;: [
</span><span style="color:#b44">      &#34;ReadWriteOnce&#34;
</span><span style="color:#b44">    ],
</span><span style="color:#b44">    &#34;resources&#34;: {
</span><span style="color:#b44">      &#34;requests&#34;: {
</span><span style="color:#b44">        &#34;storage&#34;: &#34;5Gi&#34;
</span><span style="color:#b44">      }
</span><span style="color:#b44">    }
</span><span style="color:#b44">  }
</span><span style="color:#b44">}
</span><span style="color:#b44">EOF</span></code></pre>
    </div>
    <blockquote class="note">
      <div>
        <strong>
          Note:
        </strong>
        For version 1.3+ Kubernetes will distribute dynamic PV claims across
        the configured zones. For version 1.2, dynamic persistent volumes were
        always created in the zone of the cluster master
        (here us-central1-a / us-west-2a); that issue
        (
        <a href="https://github.com/kubernetes/kubernetes/issues/23330">
          #23330
        </a>
        )
        was addressed in 1.3+.
      </div>
    </blockquote>
    <p>
      Now let&rsquo;s validate that Kubernetes automatically labeled the zone &amp; region the PV was created in.
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pv --show-labels</code></pre>
    </div>
    <p>
      The output is similar to this:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME           CAPACITY   ACCESSMODES   RECLAIM POLICY   STATUS    CLAIM            STORAGECLASS    REASON    AGE       LABELS
pv-gce-mj4gm   5Gi        RWO           Retain           Bound     default/claim1   manual                    46s       failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a</code></pre>
    </div>
    <p>
      So now we will create a pod that uses the persistent volume claim.
      Because GCE PDs / AWS EBS volumes cannot be attached across zones,
      this means that this pod can only be created in the same zone as the volume:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">kubectl<span style="color:#bbb"> </span>apply<span style="color:#bbb"> </span>-f<span style="color:#bbb"> </span>- &lt;&lt;EOF<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#a2f;font-weight:bold">apiVersion</span>:<span style="color:#bbb"> </span>v1<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#a2f;font-weight:bold">kind</span>:<span style="color:#bbb"> </span>Pod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#a2f;font-weight:bold">metadata</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#a2f;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypod<span style="color:#bbb">
</span><span style="color:#bbb"></span><span style="color:#a2f;font-weight:bold">spec</span>:<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#a2f;font-weight:bold">containers</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#a2f;font-weight:bold">name</span>:<span style="color:#bbb"> </span>myfrontend<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#a2f;font-weight:bold">image</span>:<span style="color:#bbb"> </span>nginx<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#a2f;font-weight:bold">volumeMounts</span>:<span style="color:#bbb">
</span><span style="color:#bbb">      </span>- <span style="color:#a2f;font-weight:bold">mountPath</span>:<span style="color:#bbb"> </span><span style="color:#b44">&#34;/var/www/html&#34;</span><span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#a2f;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypd<span style="color:#bbb">
</span><span style="color:#bbb">  </span><span style="color:#a2f;font-weight:bold">volumes</span>:<span style="color:#bbb">
</span><span style="color:#bbb">    </span>- <span style="color:#a2f;font-weight:bold">name</span>:<span style="color:#bbb"> </span>mypd<span style="color:#bbb">
</span><span style="color:#bbb">      </span><span style="color:#a2f;font-weight:bold">persistentVolumeClaim</span>:<span style="color:#bbb">
</span><span style="color:#bbb">        </span><span style="color:#a2f;font-weight:bold">claimName</span>:<span style="color:#bbb"> </span>claim1<span style="color:#bbb">
</span><span style="color:#bbb"></span>EOF</code></pre>
    </div>
    <p>
      Note that the pod was automatically created in the same zone as the volume, as
      cross-zone attachments are not generally permitted by cloud providers:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod mypod | grep Node</code></pre>
    </div>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Node:        kubernetes-minion-9vlv/10.240.0.5</code></pre>
    </div>
    <p>
      And check node labels:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get node kubernetes-minion-9vlv --show-labels</code></pre>
    </div>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                     STATUS    AGE    VERSION          LABELS
kubernetes-minion-9vlv   Ready     22m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-9vlv</code></pre>
    </div>
    <h3 id="pods-are-spread-across-zones">
      Pods are spread across zones
    </h3>
    <p>
      Pods in a replication controller or service are automatically spread
      across zones.  First, let&rsquo;s launch more nodes in a third zone:
    </p>
    <p>
      GCE:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBE_USE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">MULTIZONE</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>gce <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>us-central1-f <span style="color:#b8860b">NUM_NODES</span><span style="color:#666">=</span><span style="color:#666">3</span> kubernetes/cluster/kube-up.sh</code></pre>
    </div>
    <p>
      AWS:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBE_USE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">MULTIZONE</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>aws <span style="color:#b8860b">KUBE_AWS_ZONE</span><span style="color:#666">=</span>us-west-2c <span style="color:#b8860b">NUM_NODES</span><span style="color:#666">=</span><span style="color:#666">3</span> <span style="color:#b8860b">KUBE_SUBNET_CIDR</span><span style="color:#666">=</span>172.20.2.0/24 <span style="color:#b8860b">MASTER_INTERNAL_IP</span><span style="color:#666">=</span>172.20.0.9 kubernetes/cluster/kube-up.sh</code></pre>
    </div>
    <p>
      Verify that you now have nodes in 3 zones:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes --show-labels</code></pre>
    </div>
    <p>
      Create the guestbook-go example, which includes an RC of size 3, running a simple web app:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">find kubernetes/examples/guestbook-go/ -name <span style="color:#b44">&#39;*.json&#39;</span> | xargs -I <span style="color:#666">{}</span> kubectl apply -f <span style="color:#666">{}</span></code></pre>
    </div>
    <p>
      The pods should be spread across all 3 zones:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod -l <span style="color:#b8860b">app</span><span style="color:#666">=</span>guestbook | grep Node</code></pre>
    </div>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Node:        kubernetes-minion-9vlv/10.240.0.5
Node:        kubernetes-minion-281d/10.240.0.8
Node:        kubernetes-minion-olsh/10.240.0.11</code></pre>
    </div>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get node kubernetes-minion-9vlv kubernetes-minion-281d kubernetes-minion-olsh --show-labels</code></pre>
    </div>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">NAME                     STATUS    ROLES    AGE    VERSION          LABELS
kubernetes-minion-9vlv   Ready     &lt;none&gt;   34m    v1.13.0          beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-a,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-9vlv
kubernetes-minion-281d   Ready     &lt;none&gt;   20m    v1.13.0          beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-b,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-281d
kubernetes-minion-olsh   Ready     &lt;none&gt;   3m     v1.13.0          beta.kubernetes.io/instance-type<span style="color:#666">=</span>n1-standard-2,failure-domain.beta.kubernetes.io/region<span style="color:#666">=</span>us-central1,failure-domain.beta.kubernetes.io/zone<span style="color:#666">=</span>us-central1-f,kubernetes.io/hostname<span style="color:#666">=</span>kubernetes-minion-olsh</code></pre>
    </div>
    <p>
      Load-balancers span all zones in a cluster; the guestbook-go example
      includes an example load-balanced service:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe service guestbook | grep LoadBalancer.Ingress</code></pre>
    </div>
    <p>
      The output is similar to this:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">LoadBalancer Ingress:   130.211.126.21</code></pre>
    </div>
    <p>
      Set the above IP:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#a2f">export</span> <span style="color:#b8860b">IP</span><span style="color:#666">=</span>130.211.126.21</code></pre>
    </div>
    <p>
      Explore with curl via IP:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -s http://<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">IP</span><span style="color:#b68;font-weight:bold">}</span>:3000/env | grep HOSTNAME</code></pre>
    </div>
    <p>
      The output is similar to this:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">  <span style="color:#b44">&#34;HOSTNAME&#34;</span>: <span style="color:#b44">&#34;guestbook-44sep&#34;</span>,</code></pre>
    </div>
    <p>
      Again, explore multiple times:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#666">(</span><span style="color:#a2f;font-weight:bold">for</span> i in <span style="color:#b44">`</span>seq 20<span style="color:#b44">`</span>; <span style="color:#a2f;font-weight:bold">do</span> curl -s http://<span style="color:#b68;font-weight:bold">${</span><span style="color:#b8860b">IP</span><span style="color:#b68;font-weight:bold">}</span>:3000/env | grep HOSTNAME; <span style="color:#a2f;font-weight:bold">done</span><span style="color:#666">)</span>  | sort | uniq</code></pre>
    </div>
    <p>
      The output is similar to this:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">  <span style="color:#b44">&#34;HOSTNAME&#34;</span>: <span style="color:#b44">&#34;guestbook-44sep&#34;</span>,
  <span style="color:#b44">&#34;HOSTNAME&#34;</span>: <span style="color:#b44">&#34;guestbook-hum5n&#34;</span>,
  <span style="color:#b44">&#34;HOSTNAME&#34;</span>: <span style="color:#b44">&#34;guestbook-ppm40&#34;</span>,</code></pre>
    </div>
    <p>
      The load balancer correctly targets all the pods, even though they are in multiple zones.
    </p>
    <h3 id="shutting-down-the-cluster">
      Shutting down the cluster
    </h3>
    <p>
      When you&rsquo;re done, clean up:
    </p>
    <p>
      GCE:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>gce <span style="color:#b8860b">KUBE_USE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>us-central1-f kubernetes/cluster/kube-down.sh
<span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>gce <span style="color:#b8860b">KUBE_USE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>us-central1-b kubernetes/cluster/kube-down.sh
<span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>gce <span style="color:#b8860b">KUBE_GCE_ZONE</span><span style="color:#666">=</span>us-central1-a kubernetes/cluster/kube-down.sh</code></pre>
    </div>
    <p>
      AWS:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>aws <span style="color:#b8860b">KUBE_USE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBE_AWS_ZONE</span><span style="color:#666">=</span>us-west-2c kubernetes/cluster/kube-down.sh
<span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>aws <span style="color:#b8860b">KUBE_USE_EXISTING_MASTER</span><span style="color:#666">=</span><span style="color:#a2f">true</span> <span style="color:#b8860b">KUBE_AWS_ZONE</span><span style="color:#666">=</span>us-west-2b kubernetes/cluster/kube-down.sh
<span style="color:#b8860b">KUBERNETES_PROVIDER</span><span style="color:#666">=</span>aws <span style="color:#b8860b">KUBE_AWS_ZONE</span><span style="color:#666">=</span>us-west-2a kubernetes/cluster/kube-down.sh</code></pre>
    </div>
  </body>
</html>