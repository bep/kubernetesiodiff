<html><head></head><body>
    <h1>
      Reserve Compute Resources for System Daemons
    </h1>
    <p>
      <a href="https://github.com/kubernetes/website/edit/master/content/en/docs/tasks/administer-cluster/reserve-compute-resources.md" id="editPageButton" target="_blank">
        Edit This Page
      </a>
    </p>
    <h1>
      Reserve Compute Resources for System Daemons
    </h1>
    <p>
      Kubernetes nodes can be scheduled to
      <code>
        Capacity
      </code>
      . Pods can consume all the
      available capacity on a node by default. This is an issue because nodes
      typically run quite a few system daemons that power the OS and Kubernetes
      itself. Unless resources are set aside for these system daemons, pods and system
      daemons compete for resources and lead to resource starvation issues on the
      node.
    </p>
    <p>
      The
      <code>
        kubelet
      </code>
      exposes a feature named
      <code>
        Node Allocatable
      </code>
      that helps to reserve
      compute resources for system daemons. Kubernetes recommends cluster
      administrators to configure
      <code>
        Node Allocatable
      </code>
      based on their workload density
      on each node.
    </p>
    <ul id="markdown-toc">
      <li>
        <a href="#before-you-begin">
          Before you begin
        </a>
      </li>
      <li>
        <a href="#node-allocatable">
          Node Allocatable
        </a>
      </li>
      <li>
        <a href="#general-guidelines">
          General Guidelines
        </a>
      </li>
      <li>
        <a href="#example-scenario">
          Example Scenario
        </a>
      </li>
    </ul>
    <h2 id="before-you-begin">
      Before you begin
    </h2>
    <p>
      You need to have a Kubernetes cluster, and the kubectl command-line tool must
      be configured to communicate with your cluster. If you do not already have a
      cluster, you can create one by using
      <a href="/docs/setup/learning-environment/minikube/">
        Minikube
      </a>
      ,
      or you can use one of these Kubernetes playgrounds:
    </p>
    <ul>
      <li>
        <a href="https://www.katacoda.com/courses/kubernetes/playground">
          Katacoda
        </a>
      </li>
      <li>
        <a href="http://labs.play-with-k8s.com/">
          Play with Kubernetes
        </a>
      </li>
    </ul>
    <p>
      Your Kubernetes server must be at or later than version 1.8.
      To check the version, enter
      <code>
        kubectl version
      </code>
      .
    </p>
    <p>
      Your Kubernetes server must be at or later than version 1.17 to use
      the kubelet command line option
      <code>
        --reserved-cpus
      </code>
      to set an
      <a href="#explicitly-reserved-cpu-list">
        explicitly reserved CPU list
      </a>
      .
    </p>
    <h2 id="node-allocatable">
      Node Allocatable
    </h2>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">      Node Capacity
---------------------------
|     kube-reserved       |
|-------------------------|
|     system-reserved     |
|-------------------------|
|    eviction-threshold   |
|-------------------------|
|                         |
|      allocatable        |
|   (available for pods)  |
|                         |
|                         |
---------------------------</code></pre>
    </div>
    <p>
      <code>
        Allocatable
      </code>
      on a Kubernetes node is defined as the amount of compute resources
      that are available for pods. The scheduler does not over-subscribe
      <code>
        Allocatable
      </code>
      .
      <code>
        CPU
      </code>
      ,
      <code>
        memory
      </code>
      and
      <code>
        ephemeral-storage
      </code>
      are supported as of now.
    </p>
    <p>
      Node Allocatable is exposed as part of
      <code>
        v1.Node
      </code>
      object in the API and as part
      of
      <code>
        kubectl describe node
      </code>
      in the CLI.
    </p>
    <p>
      Resources can be reserved for two categories of system daemons in the
      <code>
        kubelet
      </code>
      .
    </p>
    <h3 id="enabling-qos-and-pod-level-cgroups">
      Enabling QoS and Pod level cgroups
    </h3>
    <p>
      To properly enforce node allocatable constraints on the node, you must
      enable the new cgroup hierarchy via the
      <code>
        --cgroups-per-qos
      </code>
      flag. This flag is
      enabled by default. When enabled, the
      <code>
        kubelet
      </code>
      will parent all end-user pods
      under a cgroup hierarchy managed by the
      <code>
        kubelet
      </code>
      .
    </p>
    <h3 id="configuring-a-cgroup-driver">
      Configuring a cgroup driver
    </h3>
    <p>
      The
      <code>
        kubelet
      </code>
      supports manipulation of the cgroup hierarchy on
      the host using a cgroup driver. The driver is configured via the
      <code>
        --cgroup-driver
      </code>
      flag.
    </p>
    <p>
      The supported values are the following:
    </p>
    <ul>
      <li>
        <code>
          cgroupfs
        </code>
        is the default driver that performs direct manipulation of the
        cgroup filesystem on the host in order to manage cgroup sandboxes.
      </li>
      <li>
        <code>
          systemd
        </code>
        is an alternative driver that manages cgroup sandboxes using
        transient slices for resources that are supported by that init system.
      </li>
    </ul>
    <p>
      Depending on the configuration of the associated container runtime,
      operators may have to choose a particular cgroup driver to ensure
      proper system behavior. For example, if operators use the
      <code>
        systemd
      </code>
      cgroup driver provided by the
      <code>
        docker
      </code>
      runtime, the
      <code>
        kubelet
      </code>
      must
      be configured to use the
      <code>
        systemd
      </code>
      cgroup driver.
    </p>
    <h3 id="kube-reserved">
      Kube Reserved
    </h3>
    <ul>
      <li>
        <strong>
          Kubelet Flag
        </strong>
        :
        <code>
          --kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]
        </code>
      </li>
      <li>
        <strong>
          Kubelet Flag
        </strong>
        :
        <code>
          --kube-reserved-cgroup=
        </code>
      </li>
    </ul>
    <p>
      <code>
        kube-reserved
      </code>
      is meant to capture resource reservation for kubernetes system
      daemons like the
      <code>
        kubelet
      </code>
      ,
      <code>
        container runtime
      </code>
      ,
      <code>
        node problem detector
      </code>
      , etc.
      It is not meant to reserve resources for system daemons that are run as pods.
      <code>
        kube-reserved
      </code>
      is typically a function of
      <code>
        pod density
      </code>
      on the nodes.
    </p>
    <p>
      In addition to
      <code>
        cpu
      </code>
      ,
      <code>
        memory
      </code>
      , and
      <code>
        ephemeral-storage
      </code>
      ,
      <code>
        pid
      </code>
      may be
      specified to reserve the specified number of process IDs for
      kubernetes system daemons.
    </p>
    <p>
      To optionally enforce
      <code>
        kube-reserved
      </code>
      on system daemons, specify the parent
      control group for kube daemons as the value for
      <code>
        --kube-reserved-cgroup
      </code>
      kubelet
      flag.
    </p>
    <p>
      It is recommended that the kubernetes system daemons are placed under a top
      level control group (
      <code>
        runtime.slice
      </code>
      on systemd machines for example). Each
      system daemon should ideally run within its own child control group. Refer to
      <a href="https://git.k8s.io/community/contributors/design-proposals/node/node-allocatable.md#recommended-cgroups-setup">
        this
        doc
      </a>
      for more details on recommended control group hierarchy.
    </p>
    <p>
      Note that Kubelet
      <strong>
        does not
      </strong>
      create
      <code>
        --kube-reserved-cgroup
      </code>
      if it doesn’t
      exist. Kubelet will fail if an invalid cgroup is specified.
    </p>
    <h3 id="system-reserved">
      System Reserved
    </h3>
    <ul>
      <li>
        <strong>
          Kubelet Flag
        </strong>
        :
        <code>
          --system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]
        </code>
      </li>
      <li>
        <strong>
          Kubelet Flag
        </strong>
        :
        <code>
          --system-reserved-cgroup=
        </code>
      </li>
    </ul>
    <p>
      <code>
        system-reserved
      </code>
      is meant to capture resource reservation for OS system daemons
      like
      <code>
        sshd
      </code>
      ,
      <code>
        udev
      </code>
      , etc.
      <code>
        system-reserved
      </code>
      should reserve
      <code>
        memory
      </code>
      for the
      <code>
        kernel
      </code>
      too since
      <code>
        kernel
      </code>
      memory is not accounted to pods in Kubernetes at this time.
      Reserving resources for user login sessions is also recommended (
      <code>
        user.slice
      </code>
      in
      systemd world).
    </p>
    <p>
      In addition to
      <code>
        cpu
      </code>
      ,
      <code>
        memory
      </code>
      , and
      <code>
        ephemeral-storage
      </code>
      ,
      <code>
        pid
      </code>
      may be
      specified to reserve the specified number of process IDs for OS system
      daemons.
    </p>
    <p>
      To optionally enforce
      <code>
        system-reserved
      </code>
      on system daemons, specify the parent
      control group for OS system daemons as the value for
      <code>
        --system-reserved-cgroup
      </code>
      kubelet flag.
    </p>
    <p>
      It is recommended that the OS system daemons are placed under a top level
      control group (
      <code>
        system.slice
      </code>
      on systemd machines for example).
    </p>
    <p>
      Note that Kubelet
      <strong>
        does not
      </strong>
      create
      <code>
        --system-reserved-cgroup
      </code>
      if it doesn’t
      exist. Kubelet will fail if an invalid cgroup is specified.
    </p>
    <h3 id="explicitly-reserved-cpu-list">
      Explicitly Reserved CPU List
    </h3>
    <div style="margin-top: 10px; margin-bottom: 10px;">
      <b data-diff="">
        FEATURE STATE:
      </b>
      <code>
        Kubernetes v1.17
      </code>
      <a href="#" id="feature-state-dialog-link" class="ui-state-default ui-corner-all">
        <span class="ui-icon ui-icon-newwin"></span>
        stable
      </a>
      <div id="feature-state-dialog" class="ui-dialog-content" title="stable">
        <p>
          This feature is
          <em>
            stable
          </em>
          , meaning:
        </p>
        <ul>
          <li>
            The version name is vX where X is an integer.
          </li>
          <li>
            Stable versions of features will appear in released software for many subsequent versions.
          </li>
        </ul>
      </div>
      <script>
        $(function(){

        $( "#feature-state-dialog" ).dialog({
        autoOpen: false,
        width: "600",
        buttons: [
        {
        text: "Ok",
        click: function() {
        $( this ).dialog( "close" );
        }
        }
        ]
        });


        $( "#feature-state-dialog-link" ).click(function( event ) {
        $( "#feature-state-dialog" ).dialog( "open" );
        event.preventDefault();
        });

        });
      </script>
    </div>
    <ul>
      <li>
        <strong>
          Kubelet Flag
        </strong>
        :
        <code>
          --reserved-cpus=0-3
        </code>
      </li>
    </ul>
    <p>
      <code>
        reserved-cpus
      </code>
      is meant to define an explicit CPU set for OS system daemons and
      kubernetes system daemons.
      <code>
        reserved-cpus
      </code>
      is for systems that do not intend to
      define separate top level cgroups for OS system daemons and kubernetes system daemons
      with regard to cpuset resource.
      If the Kubelet
      <strong>
        does not
      </strong>
      have
      <code>
        --system-reserved-cgroup
      </code>
      and
      <code>
        --kube-reserved-cgroup
      </code>
      ,
      the explicit cpuset provided by
      <code>
        reserved-cpus
      </code>
      will take precedence over the CPUs
      defined by
      <code>
        --kube-reserved
      </code>
      and
      <code>
        --system-reserved
      </code>
      options.
    </p>
    <p>
      This option is specifically designed for Telco/NFV use cases where uncontrolled
      interrupts/timers may impact the workload performance. you can use this option
      to define the explicit cpuset for the system/kubernetes daemons as well as the
      interrupts/timers, so the rest CPUs on the system can be used exclusively for
      workloads, with less impact from uncontrolled interrupts/timers. To move the
      system daemon, kubernetes daemons and interrupts/timers to the explicit cpuset
      defined by this option, other mechanism outside Kubernetes should be used.
      For example: in Centos, you can do this using the tuned toolset.
    </p>
    <h3 id="eviction-thresholds">
      Eviction Thresholds
    </h3>
    <ul>
      <li>
        <strong>
          Kubelet Flag
        </strong>
        :
        <code>
          --eviction-hard=[memory.available&lt;500Mi]
        </code>
      </li>
    </ul>
    <p>
      Memory pressure at the node level leads to System OOMs which affects the entire
      node and all pods running on it. Nodes can go offline temporarily until memory
      has been reclaimed. To avoid (or reduce the probability of) system OOMs kubelet
      provides
      <a href="./out-of-resource.md">
        <code>
          Out of Resource
        </code>
      </a>
      management. Evictions are
      supported for
      <code>
        memory
      </code>
      and
      <code>
        ephemeral-storage
      </code>
      only. By reserving some memory via
      <code>
        --eviction-hard
      </code>
      flag, the
      <code>
        kubelet
      </code>
      attempts to
      <code>
        evict
      </code>
      pods whenever memory
      availability on the node drops below the reserved value. Hypothetically, if
      system daemons did not exist on a node, pods cannot use more than
      <code>
        capacity -
        eviction-hard
      </code>
      . For this reason, resources reserved for evictions are not
      available for pods.
    </p>
    <h3 id="enforcing-node-allocatable">
      Enforcing Node Allocatable
    </h3>
    <ul>
      <li>
        <strong>
          Kubelet Flag
        </strong>
        :
        <code>
          --enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]
        </code>
      </li>
    </ul>
    <p>
      The scheduler treats
      <code>
        Allocatable
      </code>
      as the available
      <code>
        capacity
      </code>
      for pods.
    </p>
    <p>
      <code>
        kubelet
      </code>
      enforce
      <code>
        Allocatable
      </code>
      across pods by default. Enforcement is performed
      by evicting pods whenever the overall usage across all pods exceeds
      <code>
        Allocatable
      </code>
      . More details on eviction policy can be found
      <a href="./out-of-resource.md#eviction-policy">
        here
      </a>
      . This enforcement is controlled by
      specifying
      <code>
        pods
      </code>
      value to the kubelet flag
      <code>
        --enforce-node-allocatable
      </code>
      .
    </p>
    <p>
      Optionally,
      <code>
        kubelet
      </code>
      can be made to enforce
      <code>
        kube-reserved
      </code>
      and
      <code>
        system-reserved
      </code>
      by specifying
      <code>
        kube-reserved
      </code>
      &amp;
      <code>
        system-reserved
      </code>
      values in
      the same flag. Note that to enforce
      <code>
        kube-reserved
      </code>
      or
      <code>
        system-reserved
      </code>
      ,
      <code>
        --kube-reserved-cgroup
      </code>
      or
      <code>
        --system-reserved-cgroup
      </code>
      needs to be specified
      respectively.
    </p>
    <h2 id="general-guidelines">
      General Guidelines
    </h2>
    <p>
      System daemons are expected to be treated similar to
      <code>
        Guaranteed
      </code>
      pods. System
      daemons can burst within their bounding control groups and this behavior needs
      to be managed as part of kubernetes deployments. For example,
      <code>
        kubelet
      </code>
      should
      have its own control group and share
      <code>
        Kube-reserved
      </code>
      resources with the
      container runtime. However, Kubelet cannot burst and use up all available Node
      resources if
      <code>
        kube-reserved
      </code>
      is enforced.
    </p>
    <p>
      Be extra careful while enforcing
      <code>
        system-reserved
      </code>
      reservation since it can lead
      to critical system services being CPU starved, OOM killed, or unable
      to fork on the node. The
      recommendation is to enforce
      <code>
        system-reserved
      </code>
      only if a user has profiled their
      nodes exhaustively to come up with precise estimates and is confident in their
      ability to recover if any process in that group is oom_killed.
    </p>
    <ul>
      <li>
        To begin with enforce
        <code>
          Allocatable
        </code>
        on
        <code>
          pods
        </code>
        .
      </li>
      <li>
        Once adequate monitoring and alerting is in place to track kube system
        daemons, attempt to enforce
        <code>
          kube-reserved
        </code>
        based on usage heuristics.
      </li>
      <li>
        If absolutely necessary, enforce
        <code>
          system-reserved
        </code>
        over time.
      </li>
    </ul>
    <p>
      The resource requirements of kube system daemons may grow over time as more and
      more features are added. Over time, kubernetes project will attempt to bring
      down utilization of node system daemons, but that is not a priority as of now.
      So expect a drop in
      <code>
        Allocatable
      </code>
      capacity in future releases.
    </p>
    <h2 id="example-scenario">
      Example Scenario
    </h2>
    <p>
      Here is an example to illustrate Node Allocatable computation:
    </p>
    <ul>
      <li>
        Node has
        <code>
          32Gi
        </code>
        of
        <code>
          memory
        </code>
        ,
        <code>
          16 CPUs
        </code>
        and
        <code>
          100Gi
        </code>
        of
        <code>
          Storage
        </code>
      </li>
      <li>
        <code>
          --kube-reserved
        </code>
        is set to
        <code>
          cpu=1,memory=2Gi,ephemeral-storage=1Gi
        </code>
      </li>
      <li>
        <code>
          --system-reserved
        </code>
        is set to
        <code>
          cpu=500m,memory=1Gi,ephemeral-storage=1Gi
        </code>
      </li>
      <li>
        <code>
          --eviction-hard
        </code>
        is set to
        <code>
          memory.available&lt;500Mi,nodefs.available&lt;10%
        </code>
      </li>
    </ul>
    <p>
      Under this scenario,
      <code>
        Allocatable
      </code>
      will be
      <code>
        14.5 CPUs
      </code>
      ,
      <code>
        28.5Gi
      </code>
      of memory and
      <code>
        88Gi
      </code>
      of local storage.
      Scheduler ensures that the total memory
      <code>
        requests
      </code>
      across all pods on this node does
      not exceed
      <code>
        28.5Gi
      </code>
      and storage doesn’t exceed
      <code>
        88Gi
      </code>
      .
      Kubelet evicts pods whenever the overall memory usage across pods exceeds
      <code>
        28.5Gi
      </code>
      ,
      or if overall disk usage exceeds
      <code>
        88Gi
      </code>
      If all processes on the node consume as
      much CPU as they can, pods together cannot consume more than
      <code>
        14.5 CPUs
      </code>
      .
    </p>
    <p>
      If
      <code>
        kube-reserved
      </code>
      and/or
      <code>
        system-reserved
      </code>
      is not enforced and system daemons
      exceed their reservation,
      <code>
        kubelet
      </code>
      evicts pods whenever the overall node memory
      usage is higher than
      <code>
        31.5Gi
      </code>
      or
      <code>
        storage
      </code>
      is greater than
      <code>
        90Gi
      </code>
    </p>
  
</body></html>