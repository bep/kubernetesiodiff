<html>
  <body>
    <h1>
      Configure Out of Resource Handling
    </h1>
    <p>
      <a href="https://github.com/kubernetes/website/edit/master/content/en/docs/tasks/administer-cluster/out-of-resource.md" id="editPageButton" target="_blank">
        Edit This Page
      </a>
    </p>
    <h1>
      Configure Out of Resource Handling
    </h1>
    <p>
      This page explains how to configure out of resource handling with
      <code>
        kubelet
      </code>
      .
    </p>
    <p>
      The
      <code>
        kubelet
      </code>
      needs to preserve node stability when available compute resources
      are low. This is especially important when dealing with incompressible
      compute resources, such as memory or disk space. If such resources are exhausted,
      nodes become unstable.
    </p>
    <ul id="markdown-toc">
      <li>
        <a href="#eviction-policy">
          Eviction Policy
        </a>
      </li>
      <li>
        <a href="#node-oom-behavior">
          Node OOM Behavior
        </a>
      </li>
      <li>
        <a href="#best-practices">
          Best Practices
        </a>
      </li>
      <li>
        <a href="#deprecation-of-existing-feature-flags-to-reclaim-disk">
          Deprecation of existing feature flags to reclaim disk
        </a>
      </li>
      <li>
        <a href="#known-issues">
          Known issues
        </a>
      </li>
    </ul>
    <h2 id="eviction-policy">
      Eviction Policy
    </h2>
    <p>
      The
      <code>
        kubelet
      </code>
      can proactively monitor for and prevent total starvation of a
      compute resource. In those cases, the
      <code>
        kubelet
      </code>
      can reclaim the starved
      resource by proactively failing one or more Pods. When the
      <code>
        kubelet
      </code>
      fails
      a Pod, it terminates all of its containers and transitions its
      <code>
        PodPhase
      </code>
      to
      <code>
        Failed
      </code>
      .
      If the evicted Pod is managed by a Deployment, the Deployment will create another Pod
      to be scheduled by Kubernetes.
    </p>
    <h3 id="eviction-signals">
      Eviction Signals
    </h3>
    <p>
      The
      <code>
        kubelet
      </code>
      supports eviction decisions based on the signals described in the following
      table. The value of each signal is described in the Description column, which is based on
      the
      <code>
        kubelet
      </code>
      summary API.
    </p>
    <table>
      <thead>
        <tr>
          <th>
            Eviction Signal
          </th>
          <th>
            Description
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            <code>
              memory.available
            </code>
          </td>
          <td>
            <code>
              memory.available
            </code>
            :=
            <code>
              node.status.capacity[memory]
            </code>
            -
            <code>
              node.stats.memory.workingSet
            </code>
          </td>
        </tr>
        <tr>
          <td>
            <code>
              nodefs.available
            </code>
          </td>
          <td>
            <code>
              nodefs.available
            </code>
            :=
            <code>
              node.stats.fs.available
            </code>
          </td>
        </tr>
        <tr>
          <td>
            <code>
              nodefs.inodesFree
            </code>
          </td>
          <td>
            <code>
              nodefs.inodesFree
            </code>
            :=
            <code>
              node.stats.fs.inodesFree
            </code>
          </td>
        </tr>
        <tr>
          <td>
            <code>
              imagefs.available
            </code>
          </td>
          <td>
            <code>
              imagefs.available
            </code>
            :=
            <code>
              node.stats.runtime.imagefs.available
            </code>
          </td>
        </tr>
        <tr>
          <td>
            <code>
              imagefs.inodesFree
            </code>
          </td>
          <td>
            <code>
              imagefs.inodesFree
            </code>
            :=
            <code>
              node.stats.runtime.imagefs.inodesFree
            </code>
          </td>
        </tr>
      </tbody>
    </table>
    <p>
      Each of the above signals supports either a literal or percentage based value.
      The percentage based value is calculated relative to the total capacity
      associated with each signal.
    </p>
    <p>
      The value for
      <code>
        memory.available
      </code>
      is derived from the cgroupfs instead of tools
      like
      <code>
        free -m
      </code>
      . This is important because
      <code>
        free -m
      </code>
      does not work in a
      container, and if users use the
      <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">
        node
        allocatable
      </a>
      feature, out of resource decisions
      are made local to the end user Pod part of the cgroup hierarchy as well as the
      root node. This
      <a href="/docs/tasks/administer-cluster/out-of-resource/memory-available.sh">
        script
      </a>
      reproduces the same set of steps that the
      <code>
        kubelet
      </code>
      performs to calculate
      <code>
        memory.available
      </code>
      . The
      <code>
        kubelet
      </code>
      excludes inactive_file (i.e. # of bytes of
      file-backed memory on inactive LRU list) from its calculation as it assumes that
      memory is reclaimable under pressure.
    </p>
    <p>
      <code>
        kubelet
      </code>
      supports only two filesystem partitions.
    </p>
    <ol>
      <li>
        The
        <code>
          nodefs
        </code>
        filesystem that kubelet uses for volumes, daemon logs, etc.
      </li>
      <li>
        The
        <code>
          imagefs
        </code>
        filesystem that container runtimes uses for storing images and
        container writable layers.
      </li>
    </ol>
    <p>
      <code>
        imagefs
      </code>
      is optional.
      <code>
        kubelet
      </code>
      auto-discovers these filesystems using
      cAdvisor.
      <code>
        kubelet
      </code>
      does not care about any other filesystems. Any other types
      of configurations are not currently supported by the kubelet. For example, it is
      <em>
        not OK
      </em>
      to store volumes and logs in a dedicated
      <code>
        filesystem
      </code>
      .
    </p>
    <p>
      In future releases, the
      <code>
        kubelet
      </code>
      will deprecate the existing
      <a href="/docs/concepts/cluster-administration/kubelet-garbage-collection/">
        garbage
        collection
      </a>
      support in favor of eviction in response to disk pressure.
    </p>
    <h3 id="eviction-thresholds">
      Eviction Thresholds
    </h3>
    <p>
      The
      <code>
        kubelet
      </code>
      supports the ability to specify eviction thresholds that trigger the
      <code>
        kubelet
      </code>
      to reclaim resources.
    </p>
    <p>
      Each threshold has the following form:
    </p>
    <p>
      <code>
        [eviction-signal][operator][quantity]
      </code>
    </p>
    <p>
      where:
    </p>
    <ul>
      <li>
        <code>
          eviction-signal
        </code>
        is an eviction signal token as defined in the previous table.
      </li>
      <li>
        <code>
          operator
        </code>
        is the desired relational operator, such as
        <code>
          &lt;
        </code>
        (less than).
      </li>
      <li>
        <code>
          quantity
        </code>
        is the eviction threshold quantity, such as
        <code>
          1Gi
        </code>
        . These tokens must
        match the quantity representation used by Kubernetes. An eviction threshold can also
        be expressed as a percentage using the
        <code>
          %
        </code>
        token.
      </li>
    </ul>
    <p>
      For example, if a node has
      <code>
        10Gi
      </code>
      of total memory and you want trigger eviction if
      the available memory falls below
      <code>
        1Gi
      </code>
      , you can define the eviction threshold as
      either
      <code>
        memory.available&lt;10%
      </code>
      or
      <code>
        memory.available&lt;1Gi
      </code>
      . You cannot use both.
    </p>
    <h4 id="soft-eviction-thresholds">
      Soft Eviction Thresholds
    </h4>
    <p>
      A soft eviction threshold pairs an eviction threshold with a required
      administrator-specified grace period. No action is taken by the
      <code>
        kubelet
      </code>
      to reclaim resources associated with the eviction signal until that grace
      period has been exceeded. If no grace period is provided, the
      <code>
        kubelet
      </code>
      returns an error on startup.
    </p>
    <p>
      In addition, if a soft eviction threshold has been met, an operator can
      specify a maximum allowed Pod termination grace period to use when evicting
      pods from the node. If specified, the
      <code>
        kubelet
      </code>
      uses the lesser value among
      the
      <code>
        pod.Spec.TerminationGracePeriodSeconds
      </code>
      and the max allowed grace period.
      If not specified, the
      <code>
        kubelet
      </code>
      kills Pods immediately with no graceful
      termination.
    </p>
    <p>
      To configure soft eviction thresholds, the following flags are supported:
    </p>
    <ul>
      <li>
        <code>
          eviction-soft
        </code>
        describes a set of eviction thresholds (e.g.
        <code>
          memory.available&lt;1.5Gi
        </code>
        ) that if met over a
        corresponding grace period would trigger a Pod eviction.
      </li>
      <li>
        <code>
          eviction-soft-grace-period
        </code>
        describes a set of eviction grace periods (e.g.
        <code>
          memory.available=1m30s
        </code>
        ) that
        correspond to how long a soft eviction threshold must hold before triggering a Pod eviction.
      </li>
      <li>
        <code>
          eviction-max-pod-grace-period
        </code>
        describes the maximum allowed grace period (in seconds) to use when terminating
        pods in response to a soft eviction threshold being met.
      </li>
    </ul>
    <h4 id="hard-eviction-thresholds">
      Hard Eviction Thresholds
    </h4>
    <p>
      A hard eviction threshold has no grace period, and if observed, the
      <code>
        kubelet
      </code>
      will take immediate action to reclaim the associated starved resource. If a
      hard eviction threshold is met, the
      <code>
        kubelet
      </code>
      kills the Pod immediately
      with no graceful termination.
    </p>
    <p>
      To configure hard eviction thresholds, the following flag is supported:
    </p>
    <ul>
      <li>
        <code>
          eviction-hard
        </code>
        describes a set of eviction thresholds (e.g.
        <code>
          memory.available&lt;1Gi
        </code>
        ) that if met
        would trigger a Pod eviction.
      </li>
    </ul>
    <p>
      The
      <code>
        kubelet
      </code>
      has the following default hard eviction threshold:
    </p>
    <ul>
      <li>
        <code>
          memory.available&lt;100Mi
        </code>
      </li>
      <li>
        <code>
          nodefs.available&lt;10%
        </code>
      </li>
      <li>
        <code>
          nodefs.inodesFree&lt;5%
        </code>
      </li>
      <li>
        <code>
          imagefs.available&lt;15%
        </code>
      </li>
    </ul>
    <h3 id="eviction-monitoring-interval">
      Eviction Monitoring Interval
    </h3>
    <p>
      The
      <code>
        kubelet
      </code>
      evaluates eviction thresholds per its configured housekeeping interval.
    </p>
    <ul>
      <li>
        <code>
          housekeeping-interval
        </code>
        is the interval between container housekeepings which defaults to
        <code>
          10s
        </code>
        .
      </li>
    </ul>
    <h3 id="node-conditions">
      Node Conditions
    </h3>
    <p>
      The
      <code>
        kubelet
      </code>
      maps one or more eviction signals to a corresponding node condition.
    </p>
    <p>
      If a hard eviction threshold has been met, or a soft eviction threshold has been met
      independent of its associated grace period, the
      <code>
        kubelet
      </code>
      reports a condition that
      reflects the node is under pressure.
    </p>
    <p>
      The following node conditions are defined that correspond to the specified eviction signal.
    </p>
    <table>
      <thead>
        <tr>
          <th>
            Node Condition
          </th>
          <th>
            Eviction Signal
          </th>
          <th>
            Description
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            <code>
              MemoryPressure
            </code>
          </td>
          <td>
            <code>
              memory.available
            </code>
          </td>
          <td>
            Available memory on the node has satisfied an eviction threshold
          </td>
        </tr>
        <tr>
          <td>
            <code>
              DiskPressure
            </code>
          </td>
          <td>
            <code>
              nodefs.available
            </code>
            ,
            <code>
              nodefs.inodesFree
            </code>
            ,
            <code>
              imagefs.available
            </code>
            , or
            <code>
              imagefs.inodesFree
            </code>
          </td>
          <td>
            Available disk space and inodes on either the node&rsquo;s root filesystem or image filesystem has satisfied an eviction threshold
          </td>
        </tr>
      </tbody>
    </table>
    <p>
      The
      <code>
        kubelet
      </code>
      continues to report node status updates at the frequency specified by
      <code>
        --node-status-update-frequency
      </code>
      which defaults to
      <code>
        10s
      </code>
      .
    </p>
    <h3 id="oscillation-of-node-conditions">
      Oscillation of node conditions
    </h3>
    <p>
      If a node is oscillating above and below a soft eviction threshold, but not exceeding
      its associated grace period, it would cause the corresponding node condition to
      constantly oscillate between true and false, and could cause poor scheduling decisions
      as a consequence.
    </p>
    <p>
      To protect against this oscillation, the following flag is defined to control how
      long the
      <code>
        kubelet
      </code>
      must wait before transitioning out of a pressure condition.
    </p>
    <ul>
      <li>
        <code>
          eviction-pressure-transition-period
        </code>
        is the duration for which the
        <code>
          kubelet
        </code>
        has
        to wait before transitioning out of an eviction pressure condition.
      </li>
    </ul>
    <p>
      The
      <code>
        kubelet
      </code>
      would ensure that it has not observed an eviction threshold being met
      for the specified pressure condition for the period specified before toggling the
      condition back to
      <code>
        false
      </code>
      .
    </p>
    <h3 id="reclaiming-node-level-resources">
      Reclaiming node level resources
    </h3>
    <p>
      If an eviction threshold has been met and the grace period has passed,
      the
      <code>
        kubelet
      </code>
      initiates the process of reclaiming the pressured resource
      until it has observed the signal has gone below its defined threshold.
    </p>
    <p>
      The
      <code>
        kubelet
      </code>
      attempts to reclaim node level resources prior to evicting end-user Pods. If
      disk pressure is observed, the
      <code>
        kubelet
      </code>
      reclaims node level resources differently if the
      machine has a dedicated
      <code>
        imagefs
      </code>
      configured for the container runtime.
    </p>
    <h4 id="with-imagefs">
      With
      <code>
        imagefs
      </code>
    </h4>
    <p>
      If
      <code>
        nodefs
      </code>
      filesystem has met eviction thresholds,
      <code>
        kubelet
      </code>
      frees up disk space by deleting the dead Pods and their containers.
    </p>
    <p>
      If
      <code>
        imagefs
      </code>
      filesystem has met eviction thresholds,
      <code>
        kubelet
      </code>
      frees up disk space by deleting all unused images.
    </p>
    <h4 id="without-imagefs">
      Without
      <code>
        imagefs
      </code>
    </h4>
    <p>
      If
      <code>
        nodefs
      </code>
      filesystem has met eviction thresholds,
      <code>
        kubelet
      </code>
      frees up disk space in the following order:
    </p>
    <ol>
      <li>
        Delete dead Pods and their containers
      </li>
      <li>
        Delete all unused images
      </li>
    </ol>
    <h3 id="evicting-end-user-pods">
      Evicting end-user Pods
    </h3>
    <p>
      If the
      <code>
        kubelet
      </code>
      is unable to reclaim sufficient resource on the node,
      <code>
        kubelet
      </code>
      begins evicting Pods.
    </p>
    <p>
      The
      <code>
        kubelet
      </code>
      ranks Pods for eviction first by whether or not their usage of the starved resource exceeds requests,
      then by
      <a href="/docs/concepts/configuration/pod-priority-preemption/">
        Priority
      </a>
      , and then by the consumption of the starved compute resource relative to the Pods&rsquo; scheduling requests.
    </p>
    <p>
      As a result,
      <code>
        kubelet
      </code>
      ranks and evicts Pods in the following order:
    </p>
    <ul>
      <li>
        <code>
          BestEffort
        </code>
        or
        <code>
          Burstable
        </code>
        Pods whose usage of a starved resource exceeds its request.
        Such pods are ranked by Priority, and then usage above request.
      </li>
      <li>
        <code>
          Guaranteed
        </code>
        pods and
        <code>
          Burstable
        </code>
        pods whose usage is beneath requests are evicted last.
        <code>
          Guaranteed
        </code>
        Pods are guaranteed only when requests and limits are specified for all
        the containers and they are equal. Such pods are guaranteed to never be evicted because
        of another Pod&rsquo;s resource consumption. If a system daemon (such as
        <code>
          kubelet
        </code>
        ,
        <code>
          docker
        </code>
        ,
        and
        <code>
          journald
        </code>
        ) is consuming more resources than were reserved via
        <code>
          system-reserved
        </code>
        or
        <code>
          kube-reserved
        </code>
        allocations, and the node only has
        <code>
          Guaranteed
        </code>
        or
        <code>
          Burstable
        </code>
        Pods using
        less than requests remaining, then the node must choose to evict such a Pod in order to
        preserve node stability and to limit the impact of the unexpected consumption to other Pods.
        In this case, it will choose to evict pods of Lowest Priority first.
      </li>
    </ul>
    <p>
      If necessary,
      <code>
        kubelet
      </code>
      evicts Pods one at a time to reclaim disk when
      <code>
        DiskPressure
      </code>
      is encountered. If the
      <code>
        kubelet
      </code>
      is responding to
      <code>
        inode
      </code>
      starvation, it reclaims
      <code>
        inodes
      </code>
      by evicting Pods with the lowest quality of service first. If the
      <code>
        kubelet
      </code>
      is responding to lack of available disk, it ranks Pods within a quality of service
      that consumes the largest amount of disk and kills those first.
    </p>
    <h4 id="with-imagefs-1">
      With
      <code>
        imagefs
      </code>
    </h4>
    <p>
      If
      <code>
        nodefs
      </code>
      is triggering evictions,
      <code>
        kubelet
      </code>
      sorts Pods based on the usage on
      <code>
        nodefs
      </code>
      - local volumes + logs of all its containers.
    </p>
    <p>
      If
      <code>
        imagefs
      </code>
      is triggering evictions,
      <code>
        kubelet
      </code>
      sorts Pods based on the writable layer usage of all its containers.
    </p>
    <h4 id="without-imagefs-1">
      Without
      <code>
        imagefs
      </code>
    </h4>
    <p>
      If
      <code>
        nodefs
      </code>
      is triggering evictions,
      <code>
        kubelet
      </code>
      sorts Pods based on their total disk usage
      - local volumes + logs &amp; writable layer of all its containers.
    </p>
    <h3 id="minimum-eviction-reclaim">
      Minimum eviction reclaim
    </h3>
    <p>
      In certain scenarios, eviction of Pods could result in reclamation of small amount of resources. This can result in
      <code>
        kubelet
      </code>
      hitting eviction thresholds in repeated successions. In addition to that, eviction of resources like
      <code>
        disk
      </code>
      ,
      is time consuming.
    </p>
    <p>
      To mitigate these issues,
      <code>
        kubelet
      </code>
      can have a per-resource
      <code>
        minimum-reclaim
      </code>
      . Whenever
      <code>
        kubelet
      </code>
      observes
      resource pressure,
      <code>
        kubelet
      </code>
      attempts to reclaim at least
      <code>
        minimum-reclaim
      </code>
      amount of resource below
      the configured eviction threshold.
    </p>
    <p>
      For example, with the following configuration:
    </p>
    <pre><code>--eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi,imagefs.available&lt;100Gi
--eviction-minimum-reclaim=&#34;memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi&#34;`</code></pre>
    <p>
      If an eviction threshold is triggered for
      <code>
        memory.available
      </code>
      , the
      <code>
        kubelet
      </code>
      works to ensure
      that
      <code>
        memory.available
      </code>
      is at least
      <code>
        500Mi
      </code>
      . For
      <code>
        nodefs.available
      </code>
      , the
      <code>
        kubelet
      </code>
      works
      to ensure that
      <code>
        nodefs.available
      </code>
      is at least
      <code>
        1.5Gi
      </code>
      , and for
      <code>
        imagefs.available
      </code>
      it
      works to ensure that
      <code>
        imagefs.available
      </code>
      is at least
      <code>
        102Gi
      </code>
      before no longer reporting pressure
      on their associated resources.
    </p>
    <p>
      The default
      <code>
        eviction-minimum-reclaim
      </code>
      is
      <code>
        0
      </code>
      for all resources.
    </p>
    <h3 id="scheduler">
      Scheduler
    </h3>
    <p>
      The node reports a condition when a compute resource is under pressure. The
      scheduler views that condition as a signal to dissuade placing additional
      pods on the node.
    </p>
    <table>
      <thead>
        <tr>
          <th>
            Node Condition
          </th>
          <th>
            Scheduler Behavior
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            <code>
              MemoryPressure
            </code>
          </td>
          <td>
            No new
            <code>
              BestEffort
            </code>
            Pods are scheduled to the node.
          </td>
        </tr>
        <tr>
          <td>
            <code>
              DiskPressure
            </code>
          </td>
          <td>
            No new Pods are scheduled to the node.
          </td>
        </tr>
      </tbody>
    </table>
    <h2 id="node-oom-behavior">
      Node OOM Behavior
    </h2>
    <p>
      If the node experiences a system OOM (out of memory) event prior to the
      <code>
        kubelet
      </code>
      being able to reclaim memory,
      the node depends on the
      <a href="https://lwn.net/Articles/391222/">
        oom_killer
      </a>
      to respond.
    </p>
    <p>
      The
      <code>
        kubelet
      </code>
      sets a
      <code>
        oom_score_adj
      </code>
      value for each container based on the quality of service for the Pod.
    </p>
    <table>
      <thead>
        <tr>
          <th>
            Quality of Service
          </th>
          <th>
            oom_score_adj
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            <code>
              Guaranteed
            </code>
          </td>
          <td>
            -998
          </td>
        </tr>
        <tr>
          <td>
            <code>
              BestEffort
            </code>
          </td>
          <td>
            1000
          </td>
        </tr>
        <tr>
          <td>
            <code>
              Burstable
            </code>
          </td>
          <td>
            min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)
          </td>
        </tr>
      </tbody>
    </table>
    <p>
      If the
      <code>
        kubelet
      </code>
      is unable to reclaim memory prior to a node experiencing system OOM, the
      <code>
        oom_killer
      </code>
      calculates
      an
      <code>
        oom_score
      </code>
      based on the percentage of memory it&rsquo;s using on the node, and then add the
      <code>
        oom_score_adj
      </code>
      to get an
      effective
      <code>
        oom_score
      </code>
      for the container, and then kills the container with the highest score.
    </p>
    <p>
      The intended behavior should be that containers with the lowest quality of service that
      are consuming the largest amount of memory relative to the scheduling request should be killed first in order
      to reclaim memory.
    </p>
    <p>
      Unlike Pod eviction, if a Pod container is OOM killed, it may be restarted by the
      <code>
        kubelet
      </code>
      based on its
      <code>
        RestartPolicy
      </code>
      .
    </p>
    <h2 id="best-practices">
      Best Practices
    </h2>
    <p>
      The following sections describe best practices for out of resource handling.
    </p>
    <h3 id="schedulable-resources-and-eviction-policies">
      Schedulable resources and eviction policies
    </h3>
    <p>
      Consider the following scenario:
    </p>
    <ul>
      <li>
        Node memory capacity:
        <code>
          10Gi
        </code>
      </li>
      <li>
        Operator wants to reserve 10% of memory capacity for system daemons (kernel,
        <code>
          kubelet
        </code>
        , etc.)
      </li>
      <li>
        Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.
      </li>
    </ul>
    <p>
      To facilitate this scenario, the
      <code>
        kubelet
      </code>
      would be launched as follows:
    </p>
    <pre><code>--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi</code></pre>
    <p>
      Implicit in this configuration is the understanding that &ldquo;System reserved&rdquo; should include the amount of memory
      covered by the eviction threshold.
    </p>
    <p>
      To reach that capacity, either some Pod is using more than its request, or the system is using more than
      <code>
        1.5Gi - 500Mi = 1Gi
      </code>
      .
    </p>
    <p>
      This configuration ensures that the scheduler does not place Pods on a node that immediately induce memory pressure
      and trigger eviction assuming those Pods use less than their configured request.
    </p>
    <h3 id="daemonset">
      DaemonSet
    </h3>
    <p>
      As
      <code>
        Priority
      </code>
      is a key factor in the eviction strategy, if you do not want
      pods belonging to a
      <code>
        DaemonSet
      </code>
      to be evicted, specify a sufficiently high priorityClass
      in the pod spec template. If you want pods belonging to a
      <code>
        DaemonSet
      </code>
      to run only if
      there are sufficient resources, specify a lower or default priorityClass.
    </p>
    <h2 id="deprecation-of-existing-feature-flags-to-reclaim-disk">
      Deprecation of existing feature flags to reclaim disk
    </h2>
    <p>
      <code>
        kubelet
      </code>
      has been freeing up disk space on demand to keep the node stable.
    </p>
    <p>
      As disk based eviction matures, the following
      <code>
        kubelet
      </code>
      flags are marked for deprecation
      in favor of the simpler configuration supported around eviction.
    </p>
    <table>
      <thead>
        <tr>
          <th>
            Existing Flag
          </th>
          <th>
            New Flag
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            <code>
              --image-gc-high-threshold
            </code>
          </td>
          <td>
            <code>
              --eviction-hard
            </code>
            or
            <code>
              eviction-soft
            </code>
          </td>
        </tr>
        <tr>
          <td>
            <code>
              --image-gc-low-threshold
            </code>
          </td>
          <td>
            <code>
              --eviction-minimum-reclaim
            </code>
          </td>
        </tr>
        <tr>
          <td>
            <code>
              --maximum-dead-containers
            </code>
          </td>
          <td>
            deprecated
          </td>
        </tr>
        <tr>
          <td>
            <code>
              --maximum-dead-containers-per-container
            </code>
          </td>
          <td>
            deprecated
          </td>
        </tr>
        <tr>
          <td>
            <code>
              --minimum-container-ttl-duration
            </code>
          </td>
          <td>
            deprecated
          </td>
        </tr>
        <tr>
          <td>
            <code>
              --low-diskspace-threshold-mb
            </code>
          </td>
          <td>
            <code>
              --eviction-hard
            </code>
            or
            <code>
              eviction-soft
            </code>
          </td>
        </tr>
        <tr>
          <td>
            <code>
              --outofdisk-transition-frequency
            </code>
          </td>
          <td>
            <code>
              --eviction-pressure-transition-period
            </code>
          </td>
        </tr>
      </tbody>
    </table>
    <h2 id="known-issues">
      Known issues
    </h2>
    <p>
      The following sections describe known issues related to out of resource handling.
    </p>
    <h3 id="kubelet-may-not-observe-memory-pressure-right-away">
      kubelet may not observe memory pressure right away
    </h3>
    <p>
      The
      <code>
        kubelet
      </code>
      currently polls
      <code>
        cAdvisor
      </code>
      to collect memory usage stats at a regular interval. If memory usage
      increases within that window rapidly, the
      <code>
        kubelet
      </code>
      may not observe
      <code>
        MemoryPressure
      </code>
      fast enough, and the
      <code>
        OOMKiller
      </code>
      will still be invoked. We intend to integrate with the
      <code>
        memcg
      </code>
      notification API in a future release to reduce this
      latency, and instead have the kernel tell us when a threshold has been crossed immediately.
    </p>
    <p>
      If you are not trying to achieve extreme utilization, but a sensible measure of overcommit, a viable workaround for
      this issue is to set eviction thresholds at approximately 75% capacity. This increases the ability of this feature
      to prevent system OOMs, and promote eviction of workloads so cluster state can rebalance.
    </p>
    <h3 id="kubelet-may-evict-more-pods-than-needed">
      kubelet may evict more Pods than needed
    </h3>
    <p>
      The Pod eviction may evict more Pods than needed due to stats collection timing gap. This can be mitigated by adding
      the ability to get root container stats on an on-demand basis
      <a href="https://github.com/google/cadvisor/issues/1247">
        (https://github.com/google/cadvisor/issues/1247)
      </a>
      in the future.
    </p>
  </body>
</html>