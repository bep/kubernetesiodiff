<html><head></head><body>
    <h1>
      Troubleshoot Clusters
    </h1>
    <p>
      <a href="https://github.com/kubernetes/website/edit/master/content/en/docs/tasks/debug-application-cluster/debug-cluster.md" id="editPageButton" target="_blank">
        Edit This Page
      </a>
    </p>
    <h1>
      Troubleshoot Clusters
    </h1>
    <p>
      This doc is about cluster troubleshooting; we assume you have already ruled out your application as the root cause of the
      problem you are experiencing. See
      the
      <a href="/docs/tasks/debug-application-cluster/debug-application">
        application troubleshooting guide
      </a>
      for tips on application debugging.
      You may also visit
      <a href="/docs/troubleshooting/">
        troubleshooting document
      </a>
      for more information.
    </p>
    <ul id="markdown-toc">
      <li>
        <a href="#listing-your-cluster">
          Listing your cluster
        </a>
      </li>
      <li>
        <a href="#looking-at-logs">
          Looking at logs
        </a>
      </li>
      <li>
        <a href="#a-general-overview-of-cluster-failure-modes">
          A general overview of cluster failure modes
        </a>
      </li>
    </ul>
    <h2 id="listing-your-cluster">
      Listing your cluster
    </h2>
    <p>
      The first thing to debug in your cluster is if your nodes are all registered correctly.
    </p>
    <p>
      Run
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get nodes</code></pre>
    </div>
    <p>
      And verify that all of the nodes you expect to see are present and that they are all in the
      <code>
        Ready
      </code>
      state.
    </p>
    <p>
      To get detailed information about the overall health of your cluster, you can run:
    </p>
    <div class="highlight">
      <pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl cluster-info dump</code></pre>
    </div>
    <h2 id="looking-at-logs">
      Looking at logs
    </h2>
    <p>
      For now, digging deeper into the cluster requires logging into the relevant machines.  Here are the locations
      of the relevant log files.  (note that on systemd-based systems, you may need to use
      <code>
        journalctl
      </code>
      instead)
    </p>
    <h3 id="master">
      Master
    </h3>
    <ul>
      <li>
        <code>
          /var/log/kube-apiserver.log
        </code>
        - API Server, responsible for serving the API
      </li>
      <li>
        <code>
          /var/log/kube-scheduler.log
        </code>
        - Scheduler, responsible for making scheduling decisions
      </li>
      <li>
        <code>
          /var/log/kube-controller-manager.log
        </code>
        - Controller that manages replication controllers
      </li>
    </ul>
    <h3 id="worker-nodes">
      Worker Nodes
    </h3>
    <ul>
      <li>
        <code>
          /var/log/kubelet.log
        </code>
        - Kubelet, responsible for running containers on the node
      </li>
      <li>
        <code>
          /var/log/kube-proxy.log
        </code>
        - Kube Proxy, responsible for service load balancing
      </li>
    </ul>
    <h2 id="a-general-overview-of-cluster-failure-modes">
      A general overview of cluster failure modes
    </h2>
    <p>
      This is an incomplete list of things that could go wrong, and how to adjust your cluster setup to mitigate the problems.
    </p>
    <h3 id="root-causes">
      Root causes:
    </h3>
    <ul>
      <li>
        VM(s) shutdown
      </li>
      <li>
        Network partition within cluster, or between cluster and users
      </li>
      <li>
        Crashes in Kubernetes software
      </li>
      <li>
        Data loss or unavailability of persistent storage (e.g. GCE PD or AWS EBS volume)
      </li>
      <li>
        Operator error, for example misconfigured Kubernetes software or application software
      </li>
    </ul>
    <h3 id="specific-scenarios">
      Specific scenarios:
    </h3>
    <ul>
      <li>
        Apiserver VM shutdown or apiserver crashing
        <ul>
          <li>
            Results
            <ul>
              <li>
                unable to stop, update, or start new pods, services, replication controller
              </li>
              <li>
                existing pods and services should continue to work normally, unless they depend on the Kubernetes API
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        Apiserver backing storage lost
        <ul>
          <li>
            Results
            <ul>
              <li>
                apiserver should fail to come up
              </li>
              <li>
                kubelets will not be able to reach it but will continue to run the same pods and provide the same service proxying
              </li>
              <li>
                manual recovery or recreation of apiserver state necessary before apiserver is restarted
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        Supporting services (node controller, replication controller manager, scheduler, etc) VM shutdown or crashes
        <ul>
          <li>
            currently those are colocated with the apiserver, and their unavailability has similar consequences as apiserver
          </li>
          <li>
            in future, these will be replicated as well and may not be co-located
          </li>
          <li>
            they do not have their own persistent state
          </li>
        </ul>
      </li>
      <li>
        Individual node (VM or physical machine) shuts down
        <ul>
          <li>
            Results
            <ul>
              <li>
                pods on that Node stop running
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        Network partition
        <ul>
          <li>
            Results
            <ul>
              <li>
                partition A thinks the nodes in partition B are down; partition B thinks the apiserver is down. (Assuming the master VM ends up in partition A.)
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        Kubelet software fault
        <ul>
          <li>
            Results
            <ul>
              <li>
                crashing kubelet cannot start new pods on the node
              </li>
              <li>
                kubelet might delete the pods or not
              </li>
              <li>
                node marked unhealthy
              </li>
              <li>
                replication controllers start new pods elsewhere
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        Cluster operator error
        <ul>
          <li>
            Results
            <ul>
              <li>
                loss of pods, services, etc
              </li>
              <li>
                lost of apiserver backing store
              </li>
              <li>
                users unable to read API
              </li>
              <li>
                etc.
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h3 id="mitigations">
      Mitigations:
    </h3>
    <ul>
      <li>
        <p>
          Action: Use IaaS provider’s automatic VM restarting feature for IaaS VMs
        </p>
        <ul>
          <li>
            Mitigates: Apiserver VM shutdown or apiserver crashing
          </li>
          <li>
            Mitigates: Supporting services VM shutdown or crashes
          </li>
        </ul>
      </li>
      <li>
        <p>
          Action: Use IaaS providers reliable storage (e.g. GCE PD or AWS EBS volume) for VMs with apiserver+etcd
        </p>
        <ul>
          <li>
            Mitigates: Apiserver backing storage lost
          </li>
        </ul>
      </li>
      <li>
        <p>
          Action: Use
          <a href="/docs/admin/high-availability">
            high-availability
          </a>
          configuration
        </p>
        <ul>
          <li>
            Mitigates: Control plane node shutdown or control plane components (scheduler, API server, controller-manager) crashing
          </li>
          <li>
            Will tolerate one or more simultaneous node or component failures
          </li>
          <li data-diff="">
            Mitigates: API server backing storage (i.e., etcd’s data directory) lost
          </li>
          <li>
            Assumes HA (highly-available) etcd configuration
          </li>
        </ul>
      </li>
      <li>
        <p>
          Action: Snapshot apiserver PDs/EBS-volumes periodically
        </p>
        <ul>
          <li>
            Mitigates: Apiserver backing storage lost
          </li>
          <li>
            Mitigates: Some cases of operator error
          </li>
          <li>
            Mitigates: Some cases of Kubernetes software fault
          </li>
        </ul>
      </li>
      <li>
        <p>
          Action: use replication controller and services in front of pods
        </p>
        <ul>
          <li>
            Mitigates: Node shutdown
          </li>
          <li>
            Mitigates: Kubelet software fault
          </li>
        </ul>
      </li>
      <li>
        <p>
          Action: applications (containers) designed to tolerate unexpected restarts
        </p>
        <ul>
          <li>
            Mitigates: Node shutdown
          </li>
          <li>
            Mitigates: Kubelet software fault
          </li>
        </ul>
      </li>
    </ul>
  
</body></html>